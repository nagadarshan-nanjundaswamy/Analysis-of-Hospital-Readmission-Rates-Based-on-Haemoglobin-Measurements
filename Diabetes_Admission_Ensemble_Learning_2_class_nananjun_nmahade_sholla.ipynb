{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diabetes_Admission_Ensemble_Learning_2_class_nananjun_nmahade_sholla.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxjLsmGBKmre"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('diabetic_data.csv')\n",
        "df = df.drop(['encounter_id', 'patient_nbr', \n",
        "            'weight', 'payer_code', 'medical_specialty', \n",
        "            'diag_1', 'diag_2', 'diag_3'], axis=1)\n",
        "# df = df.loc[:, ~(df == '?').any()]\n",
        "df_x = df.drop(['readmitted'], axis=1)\n",
        "df_y = df['readmitted']\n",
        "y_full = np.array(df_y.values)\n",
        "y_full[y_full=='NO'] = 0\n",
        "y_full[y_full=='>30'] = 1\n",
        "y_full[y_full=='<30'] = 1\n",
        "y_full = np.array(y_full).astype('int32')\n",
        "categorical_variables = df_x.select_dtypes(include='object').columns.tolist()\n",
        "continuous_variables = df_x.select_dtypes(exclude='object').columns.tolist()\n",
        "\n",
        "ct_ss_ohe = ColumnTransformer(\n",
        "    [('scaling', StandardScaler(), continuous_variables), \n",
        "    ('onehot', OneHotEncoder(sparse=False), categorical_variables)])\n",
        "x_full = ct_ss_ohe.fit_transform(df_x)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_full, y_full, stratify=y_full, random_state=0)"
      ],
      "metadata": {
        "id": "uxrJqjUbL1Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (x_train.shape)\n",
        "print (y_train.shape)\n",
        "print (x_test.shape)\n",
        "print (y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L18UVFelago1",
        "outputId": "a6d7c673-2c14-4ebc-ffe5-194dbc8803af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(76324, 113)\n",
            "(76324,)\n",
            "(25442, 113)\n",
            "(25442,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    References:\n",
        "    https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html\n",
        "    https://discuss.pytorch.org/t/whats-the-difference-between-nn-relu-vs-f-relu/27599/2\n",
        "    https://youtu.be/oPhxf2fXHkQ\n",
        "    https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
        "    https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "    https://pytorch.org/tutorials/beginner/basics/intro.html\n",
        "    https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "    https://youtu.be/PXOzkkB5eH0\n",
        "\"\"\"\n",
        "\n",
        "class DiabetesDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = torch.from_numpy(x).float()\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "        self.n_samples = x.shape[0]\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index], self.y[index]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "    \n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden_layer = nn.Linear(113, 25)\n",
        "        self.output_layer = nn.Linear(25, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layer(x)\n",
        "        h_f = self.relu(x)\n",
        "        x = self.output_layer(h_f)\n",
        "        return x, h_f\n",
        "\n",
        "class Model():\n",
        "    def __init__(self, model, optimizer, loss_function):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_function = loss_function\n",
        "    \n",
        "    def train(self, train_loader, epochs):\n",
        "        self.train_loader = train_loader\n",
        "        for epoch in range(epochs):\n",
        "            for i, data in enumerate(train_loader):\n",
        "                inputs = data[0].to(device)\n",
        "                labels = data[1].to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs, h_f = self.model(inputs)\n",
        "                loss = self.loss_function(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                if (i+1)%100==0:\n",
        "                    print (f'Epoch: {epoch+1}/{epochs}\\t', end='', flush=True)\n",
        "                    print (f'Batch: {i+1}/{len(train_loader)}\\t', end='', flush=True)\n",
        "                    print (f'Loss: {loss.item()}', flush=True)\n",
        "            print ('=======================================================================================')\n",
        "        return\n",
        "\n",
        "    def test(self, test_loader):\n",
        "        self.test_loader = test_loader\n",
        "        probs = np.array([])\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for i, data in enumerate(test_loader):\n",
        "                inputs = data[0].to(device)\n",
        "                labels = data[1].to(device)\n",
        "                outputs, h_f = self.model(inputs)\n",
        "                if i==0:\n",
        "                    probs = F.softmax(outputs, dim=1).numpy()\n",
        "                else:\n",
        "                    probs = np.vstack( (probs, F.softmax(outputs, dim=1).numpy() ))\n",
        "                _, batch_pred = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (batch_pred==labels).sum().item()\n",
        "        accuracy = (correct/total)*100\n",
        "        return accuracy, probs\n",
        "    \n",
        "    def feature_extractor(self, data_loader):\n",
        "        extracted_data = np.array([])\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            for i, data in enumerate(data_loader):\n",
        "                inputs = data[0].to(device)\n",
        "                labels = data[1].to(device)\n",
        "                outputs, h_f = self.model(inputs)\n",
        "                if i==0:\n",
        "                    extracted_data = np.column_stack((h_f.numpy(), labels.numpy()))\n",
        "                else:\n",
        "                    extracted_data = np.vstack( (extracted_data, np.column_stack( ( h_f.numpy(), labels.numpy() ) ) ))\n",
        "        return extracted_data"
      ],
      "metadata": {
        "id": "zAJ_FmlNJZDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "lr = 0.001\n",
        "epochs=100"
      ],
      "metadata": {
        "id": "HreB5qs5KQeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    References:\n",
        "    https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "    https://pytorch.org/docs/stable/data.html\n",
        "\"\"\"\n",
        "train_data = DiabetesDataset(x_train, y_train)\n",
        "test_data = DiabetesDataset(x_test, y_test)"
      ],
      "metadata": {
        "id": "g8nmNgalJ1Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "JFHgfLQSKgNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net().to(device)\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "model = Model(net, optimizer, loss_function)"
      ],
      "metadata": {
        "id": "WI7rN9nq2bTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train(train_loader=train_loader, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVr-94L-EP0u",
        "outputId": "5bb411fa-d830-4304-b1f0-e309e93f1eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100\tBatch: 100/299\tLoss: 0.6272361874580383\n",
            "Epoch: 1/100\tBatch: 200/299\tLoss: 0.6332648992538452\n",
            "=======================================================================================\n",
            "Epoch: 2/100\tBatch: 100/299\tLoss: 0.6199131608009338\n",
            "Epoch: 2/100\tBatch: 200/299\tLoss: 0.6527225375175476\n",
            "=======================================================================================\n",
            "Epoch: 3/100\tBatch: 100/299\tLoss: 0.6656703948974609\n",
            "Epoch: 3/100\tBatch: 200/299\tLoss: 0.6196946501731873\n",
            "=======================================================================================\n",
            "Epoch: 4/100\tBatch: 100/299\tLoss: 0.6295291781425476\n",
            "Epoch: 4/100\tBatch: 200/299\tLoss: 0.6226063370704651\n",
            "=======================================================================================\n",
            "Epoch: 5/100\tBatch: 100/299\tLoss: 0.6568787097930908\n",
            "Epoch: 5/100\tBatch: 200/299\tLoss: 0.6696155667304993\n",
            "=======================================================================================\n",
            "Epoch: 6/100\tBatch: 100/299\tLoss: 0.6113044619560242\n",
            "Epoch: 6/100\tBatch: 200/299\tLoss: 0.6545770764350891\n",
            "=======================================================================================\n",
            "Epoch: 7/100\tBatch: 100/299\tLoss: 0.6472380757331848\n",
            "Epoch: 7/100\tBatch: 200/299\tLoss: 0.6289876103401184\n",
            "=======================================================================================\n",
            "Epoch: 8/100\tBatch: 100/299\tLoss: 0.6304786205291748\n",
            "Epoch: 8/100\tBatch: 200/299\tLoss: 0.6574523448944092\n",
            "=======================================================================================\n",
            "Epoch: 9/100\tBatch: 100/299\tLoss: 0.6043490767478943\n",
            "Epoch: 9/100\tBatch: 200/299\tLoss: 0.6112903952598572\n",
            "=======================================================================================\n",
            "Epoch: 10/100\tBatch: 100/299\tLoss: 0.6245519518852234\n",
            "Epoch: 10/100\tBatch: 200/299\tLoss: 0.6152167320251465\n",
            "=======================================================================================\n",
            "Epoch: 11/100\tBatch: 100/299\tLoss: 0.6512819528579712\n",
            "Epoch: 11/100\tBatch: 200/299\tLoss: 0.6302250623703003\n",
            "=======================================================================================\n",
            "Epoch: 12/100\tBatch: 100/299\tLoss: 0.6589452028274536\n",
            "Epoch: 12/100\tBatch: 200/299\tLoss: 0.6405894756317139\n",
            "=======================================================================================\n",
            "Epoch: 13/100\tBatch: 100/299\tLoss: 0.6463649272918701\n",
            "Epoch: 13/100\tBatch: 200/299\tLoss: 0.6192868947982788\n",
            "=======================================================================================\n",
            "Epoch: 14/100\tBatch: 100/299\tLoss: 0.646541178226471\n",
            "Epoch: 14/100\tBatch: 200/299\tLoss: 0.626198947429657\n",
            "=======================================================================================\n",
            "Epoch: 15/100\tBatch: 100/299\tLoss: 0.623302161693573\n",
            "Epoch: 15/100\tBatch: 200/299\tLoss: 0.6395028829574585\n",
            "=======================================================================================\n",
            "Epoch: 16/100\tBatch: 100/299\tLoss: 0.6172500252723694\n",
            "Epoch: 16/100\tBatch: 200/299\tLoss: 0.6488800644874573\n",
            "=======================================================================================\n",
            "Epoch: 17/100\tBatch: 100/299\tLoss: 0.6026639342308044\n",
            "Epoch: 17/100\tBatch: 200/299\tLoss: 0.6045386791229248\n",
            "=======================================================================================\n",
            "Epoch: 18/100\tBatch: 100/299\tLoss: 0.6288822293281555\n",
            "Epoch: 18/100\tBatch: 200/299\tLoss: 0.642533004283905\n",
            "=======================================================================================\n",
            "Epoch: 19/100\tBatch: 100/299\tLoss: 0.5965237617492676\n",
            "Epoch: 19/100\tBatch: 200/299\tLoss: 0.6392830014228821\n",
            "=======================================================================================\n",
            "Epoch: 20/100\tBatch: 100/299\tLoss: 0.6082338690757751\n",
            "Epoch: 20/100\tBatch: 200/299\tLoss: 0.6312597393989563\n",
            "=======================================================================================\n",
            "Epoch: 21/100\tBatch: 100/299\tLoss: 0.6315973401069641\n",
            "Epoch: 21/100\tBatch: 200/299\tLoss: 0.6251618266105652\n",
            "=======================================================================================\n",
            "Epoch: 22/100\tBatch: 100/299\tLoss: 0.6644692420959473\n",
            "Epoch: 22/100\tBatch: 200/299\tLoss: 0.6367508769035339\n",
            "=======================================================================================\n",
            "Epoch: 23/100\tBatch: 100/299\tLoss: 0.6096267104148865\n",
            "Epoch: 23/100\tBatch: 200/299\tLoss: 0.6450421810150146\n",
            "=======================================================================================\n",
            "Epoch: 24/100\tBatch: 100/299\tLoss: 0.6244537830352783\n",
            "Epoch: 24/100\tBatch: 200/299\tLoss: 0.62959223985672\n",
            "=======================================================================================\n",
            "Epoch: 25/100\tBatch: 100/299\tLoss: 0.6422477960586548\n",
            "Epoch: 25/100\tBatch: 200/299\tLoss: 0.6560630202293396\n",
            "=======================================================================================\n",
            "Epoch: 26/100\tBatch: 100/299\tLoss: 0.6337763667106628\n",
            "Epoch: 26/100\tBatch: 200/299\tLoss: 0.6396406888961792\n",
            "=======================================================================================\n",
            "Epoch: 27/100\tBatch: 100/299\tLoss: 0.6208339929580688\n",
            "Epoch: 27/100\tBatch: 200/299\tLoss: 0.5956154465675354\n",
            "=======================================================================================\n",
            "Epoch: 28/100\tBatch: 100/299\tLoss: 0.6001219153404236\n",
            "Epoch: 28/100\tBatch: 200/299\tLoss: 0.6338990330696106\n",
            "=======================================================================================\n",
            "Epoch: 29/100\tBatch: 100/299\tLoss: 0.6358904838562012\n",
            "Epoch: 29/100\tBatch: 200/299\tLoss: 0.6276451945304871\n",
            "=======================================================================================\n",
            "Epoch: 30/100\tBatch: 100/299\tLoss: 0.610509991645813\n",
            "Epoch: 30/100\tBatch: 200/299\tLoss: 0.623315155506134\n",
            "=======================================================================================\n",
            "Epoch: 31/100\tBatch: 100/299\tLoss: 0.6008586287498474\n",
            "Epoch: 31/100\tBatch: 200/299\tLoss: 0.6121962070465088\n",
            "=======================================================================================\n",
            "Epoch: 32/100\tBatch: 100/299\tLoss: 0.6166533827781677\n",
            "Epoch: 32/100\tBatch: 200/299\tLoss: 0.6370007991790771\n",
            "=======================================================================================\n",
            "Epoch: 33/100\tBatch: 100/299\tLoss: 0.6458917260169983\n",
            "Epoch: 33/100\tBatch: 200/299\tLoss: 0.6423383951187134\n",
            "=======================================================================================\n",
            "Epoch: 34/100\tBatch: 100/299\tLoss: 0.6116489171981812\n",
            "Epoch: 34/100\tBatch: 200/299\tLoss: 0.6067121028900146\n",
            "=======================================================================================\n",
            "Epoch: 35/100\tBatch: 100/299\tLoss: 0.6342009902000427\n",
            "Epoch: 35/100\tBatch: 200/299\tLoss: 0.64976567029953\n",
            "=======================================================================================\n",
            "Epoch: 36/100\tBatch: 100/299\tLoss: 0.6013264656066895\n",
            "Epoch: 36/100\tBatch: 200/299\tLoss: 0.6248282194137573\n",
            "=======================================================================================\n",
            "Epoch: 37/100\tBatch: 100/299\tLoss: 0.6118613481521606\n",
            "Epoch: 37/100\tBatch: 200/299\tLoss: 0.629447877407074\n",
            "=======================================================================================\n",
            "Epoch: 38/100\tBatch: 100/299\tLoss: 0.6488941311836243\n",
            "Epoch: 38/100\tBatch: 200/299\tLoss: 0.590114414691925\n",
            "=======================================================================================\n",
            "Epoch: 39/100\tBatch: 100/299\tLoss: 0.6530897617340088\n",
            "Epoch: 39/100\tBatch: 200/299\tLoss: 0.6471096277236938\n",
            "=======================================================================================\n",
            "Epoch: 40/100\tBatch: 100/299\tLoss: 0.6098049283027649\n",
            "Epoch: 40/100\tBatch: 200/299\tLoss: 0.6092378497123718\n",
            "=======================================================================================\n",
            "Epoch: 41/100\tBatch: 100/299\tLoss: 0.5764339566230774\n",
            "Epoch: 41/100\tBatch: 200/299\tLoss: 0.5944568514823914\n",
            "=======================================================================================\n",
            "Epoch: 42/100\tBatch: 100/299\tLoss: 0.6271998882293701\n",
            "Epoch: 42/100\tBatch: 200/299\tLoss: 0.6500065922737122\n",
            "=======================================================================================\n",
            "Epoch: 43/100\tBatch: 100/299\tLoss: 0.6514068245887756\n",
            "Epoch: 43/100\tBatch: 200/299\tLoss: 0.6123152375221252\n",
            "=======================================================================================\n",
            "Epoch: 44/100\tBatch: 100/299\tLoss: 0.6113220453262329\n",
            "Epoch: 44/100\tBatch: 200/299\tLoss: 0.6284555196762085\n",
            "=======================================================================================\n",
            "Epoch: 45/100\tBatch: 100/299\tLoss: 0.6533693075180054\n",
            "Epoch: 45/100\tBatch: 200/299\tLoss: 0.5867998003959656\n",
            "=======================================================================================\n",
            "Epoch: 46/100\tBatch: 100/299\tLoss: 0.6129695177078247\n",
            "Epoch: 46/100\tBatch: 200/299\tLoss: 0.621186375617981\n",
            "=======================================================================================\n",
            "Epoch: 47/100\tBatch: 100/299\tLoss: 0.6333343386650085\n",
            "Epoch: 47/100\tBatch: 200/299\tLoss: 0.6481100916862488\n",
            "=======================================================================================\n",
            "Epoch: 48/100\tBatch: 100/299\tLoss: 0.6401368975639343\n",
            "Epoch: 48/100\tBatch: 200/299\tLoss: 0.5945346355438232\n",
            "=======================================================================================\n",
            "Epoch: 49/100\tBatch: 100/299\tLoss: 0.6060637831687927\n",
            "Epoch: 49/100\tBatch: 200/299\tLoss: 0.5999100804328918\n",
            "=======================================================================================\n",
            "Epoch: 50/100\tBatch: 100/299\tLoss: 0.6049402952194214\n",
            "Epoch: 50/100\tBatch: 200/299\tLoss: 0.6154361963272095\n",
            "=======================================================================================\n",
            "Epoch: 51/100\tBatch: 100/299\tLoss: 0.6256070733070374\n",
            "Epoch: 51/100\tBatch: 200/299\tLoss: 0.5922779440879822\n",
            "=======================================================================================\n",
            "Epoch: 52/100\tBatch: 100/299\tLoss: 0.6245464086532593\n",
            "Epoch: 52/100\tBatch: 200/299\tLoss: 0.6177275776863098\n",
            "=======================================================================================\n",
            "Epoch: 53/100\tBatch: 100/299\tLoss: 0.6303319931030273\n",
            "Epoch: 53/100\tBatch: 200/299\tLoss: 0.6380866169929504\n",
            "=======================================================================================\n",
            "Epoch: 54/100\tBatch: 100/299\tLoss: 0.6215070486068726\n",
            "Epoch: 54/100\tBatch: 200/299\tLoss: 0.619460940361023\n",
            "=======================================================================================\n",
            "Epoch: 55/100\tBatch: 100/299\tLoss: 0.6558752655982971\n",
            "Epoch: 55/100\tBatch: 200/299\tLoss: 0.6376628875732422\n",
            "=======================================================================================\n",
            "Epoch: 56/100\tBatch: 100/299\tLoss: 0.6240670084953308\n",
            "Epoch: 56/100\tBatch: 200/299\tLoss: 0.5857630968093872\n",
            "=======================================================================================\n",
            "Epoch: 57/100\tBatch: 100/299\tLoss: 0.6197352409362793\n",
            "Epoch: 57/100\tBatch: 200/299\tLoss: 0.5937469005584717\n",
            "=======================================================================================\n",
            "Epoch: 58/100\tBatch: 100/299\tLoss: 0.6623257398605347\n",
            "Epoch: 58/100\tBatch: 200/299\tLoss: 0.652723491191864\n",
            "=======================================================================================\n",
            "Epoch: 59/100\tBatch: 100/299\tLoss: 0.6139129400253296\n",
            "Epoch: 59/100\tBatch: 200/299\tLoss: 0.5925129055976868\n",
            "=======================================================================================\n",
            "Epoch: 60/100\tBatch: 100/299\tLoss: 0.6423331499099731\n",
            "Epoch: 60/100\tBatch: 200/299\tLoss: 0.5908169150352478\n",
            "=======================================================================================\n",
            "Epoch: 61/100\tBatch: 100/299\tLoss: 0.5837332606315613\n",
            "Epoch: 61/100\tBatch: 200/299\tLoss: 0.6492449641227722\n",
            "=======================================================================================\n",
            "Epoch: 62/100\tBatch: 100/299\tLoss: 0.6050999164581299\n",
            "Epoch: 62/100\tBatch: 200/299\tLoss: 0.6390939354896545\n",
            "=======================================================================================\n",
            "Epoch: 63/100\tBatch: 100/299\tLoss: 0.617465615272522\n",
            "Epoch: 63/100\tBatch: 200/299\tLoss: 0.6326456069946289\n",
            "=======================================================================================\n",
            "Epoch: 64/100\tBatch: 100/299\tLoss: 0.6221843361854553\n",
            "Epoch: 64/100\tBatch: 200/299\tLoss: 0.6380776762962341\n",
            "=======================================================================================\n",
            "Epoch: 65/100\tBatch: 100/299\tLoss: 0.6259713768959045\n",
            "Epoch: 65/100\tBatch: 200/299\tLoss: 0.6126766204833984\n",
            "=======================================================================================\n",
            "Epoch: 66/100\tBatch: 100/299\tLoss: 0.6468777656555176\n",
            "Epoch: 66/100\tBatch: 200/299\tLoss: 0.6368147730827332\n",
            "=======================================================================================\n",
            "Epoch: 67/100\tBatch: 100/299\tLoss: 0.5740100145339966\n",
            "Epoch: 67/100\tBatch: 200/299\tLoss: 0.6363265514373779\n",
            "=======================================================================================\n",
            "Epoch: 68/100\tBatch: 100/299\tLoss: 0.6319214105606079\n",
            "Epoch: 68/100\tBatch: 200/299\tLoss: 0.605872631072998\n",
            "=======================================================================================\n",
            "Epoch: 69/100\tBatch: 100/299\tLoss: 0.6131061315536499\n",
            "Epoch: 69/100\tBatch: 200/299\tLoss: 0.6521127223968506\n",
            "=======================================================================================\n",
            "Epoch: 70/100\tBatch: 100/299\tLoss: 0.6299144625663757\n",
            "Epoch: 70/100\tBatch: 200/299\tLoss: 0.6216716170310974\n",
            "=======================================================================================\n",
            "Epoch: 71/100\tBatch: 100/299\tLoss: 0.6361196637153625\n",
            "Epoch: 71/100\tBatch: 200/299\tLoss: 0.6530069708824158\n",
            "=======================================================================================\n",
            "Epoch: 72/100\tBatch: 100/299\tLoss: 0.6633252501487732\n",
            "Epoch: 72/100\tBatch: 200/299\tLoss: 0.6364733576774597\n",
            "=======================================================================================\n",
            "Epoch: 73/100\tBatch: 100/299\tLoss: 0.6020358204841614\n",
            "Epoch: 73/100\tBatch: 200/299\tLoss: 0.6381838321685791\n",
            "=======================================================================================\n",
            "Epoch: 74/100\tBatch: 100/299\tLoss: 0.5979571342468262\n",
            "Epoch: 74/100\tBatch: 200/299\tLoss: 0.642266571521759\n",
            "=======================================================================================\n",
            "Epoch: 75/100\tBatch: 100/299\tLoss: 0.6303070187568665\n",
            "Epoch: 75/100\tBatch: 200/299\tLoss: 0.605016827583313\n",
            "=======================================================================================\n",
            "Epoch: 76/100\tBatch: 100/299\tLoss: 0.605814516544342\n",
            "Epoch: 76/100\tBatch: 200/299\tLoss: 0.6174008250236511\n",
            "=======================================================================================\n",
            "Epoch: 77/100\tBatch: 100/299\tLoss: 0.601947546005249\n",
            "Epoch: 77/100\tBatch: 200/299\tLoss: 0.6360840201377869\n",
            "=======================================================================================\n",
            "Epoch: 78/100\tBatch: 100/299\tLoss: 0.6111553311347961\n",
            "Epoch: 78/100\tBatch: 200/299\tLoss: 0.5995725989341736\n",
            "=======================================================================================\n",
            "Epoch: 79/100\tBatch: 100/299\tLoss: 0.5928801894187927\n",
            "Epoch: 79/100\tBatch: 200/299\tLoss: 0.5943295955657959\n",
            "=======================================================================================\n",
            "Epoch: 80/100\tBatch: 100/299\tLoss: 0.5947243571281433\n",
            "Epoch: 80/100\tBatch: 200/299\tLoss: 0.6265522837638855\n",
            "=======================================================================================\n",
            "Epoch: 81/100\tBatch: 100/299\tLoss: 0.6150375604629517\n",
            "Epoch: 81/100\tBatch: 200/299\tLoss: 0.6156097650527954\n",
            "=======================================================================================\n",
            "Epoch: 82/100\tBatch: 100/299\tLoss: 0.6019598841667175\n",
            "Epoch: 82/100\tBatch: 200/299\tLoss: 0.5924729108810425\n",
            "=======================================================================================\n",
            "Epoch: 83/100\tBatch: 100/299\tLoss: 0.5831577777862549\n",
            "Epoch: 83/100\tBatch: 200/299\tLoss: 0.6389929056167603\n",
            "=======================================================================================\n",
            "Epoch: 84/100\tBatch: 100/299\tLoss: 0.6288471817970276\n",
            "Epoch: 84/100\tBatch: 200/299\tLoss: 0.5930507779121399\n",
            "=======================================================================================\n",
            "Epoch: 85/100\tBatch: 100/299\tLoss: 0.5798920392990112\n",
            "Epoch: 85/100\tBatch: 200/299\tLoss: 0.6544241905212402\n",
            "=======================================================================================\n",
            "Epoch: 86/100\tBatch: 100/299\tLoss: 0.5991935729980469\n",
            "Epoch: 86/100\tBatch: 200/299\tLoss: 0.6342832446098328\n",
            "=======================================================================================\n",
            "Epoch: 87/100\tBatch: 100/299\tLoss: 0.6143584847450256\n",
            "Epoch: 87/100\tBatch: 200/299\tLoss: 0.62303227186203\n",
            "=======================================================================================\n",
            "Epoch: 88/100\tBatch: 100/299\tLoss: 0.5925618410110474\n",
            "Epoch: 88/100\tBatch: 200/299\tLoss: 0.6508026123046875\n",
            "=======================================================================================\n",
            "Epoch: 89/100\tBatch: 100/299\tLoss: 0.6449724435806274\n",
            "Epoch: 89/100\tBatch: 200/299\tLoss: 0.5995578169822693\n",
            "=======================================================================================\n",
            "Epoch: 90/100\tBatch: 100/299\tLoss: 0.5938688516616821\n",
            "Epoch: 90/100\tBatch: 200/299\tLoss: 0.6539474129676819\n",
            "=======================================================================================\n",
            "Epoch: 91/100\tBatch: 100/299\tLoss: 0.6515913009643555\n",
            "Epoch: 91/100\tBatch: 200/299\tLoss: 0.6046675443649292\n",
            "=======================================================================================\n",
            "Epoch: 92/100\tBatch: 100/299\tLoss: 0.6093911528587341\n",
            "Epoch: 92/100\tBatch: 200/299\tLoss: 0.6463038325309753\n",
            "=======================================================================================\n",
            "Epoch: 93/100\tBatch: 100/299\tLoss: 0.6926748752593994\n",
            "Epoch: 93/100\tBatch: 200/299\tLoss: 0.6601201295852661\n",
            "=======================================================================================\n",
            "Epoch: 94/100\tBatch: 100/299\tLoss: 0.6168987154960632\n",
            "Epoch: 94/100\tBatch: 200/299\tLoss: 0.5915995836257935\n",
            "=======================================================================================\n",
            "Epoch: 95/100\tBatch: 100/299\tLoss: 0.5656846165657043\n",
            "Epoch: 95/100\tBatch: 200/299\tLoss: 0.6292985677719116\n",
            "=======================================================================================\n",
            "Epoch: 96/100\tBatch: 100/299\tLoss: 0.6058644652366638\n",
            "Epoch: 96/100\tBatch: 200/299\tLoss: 0.6359878182411194\n",
            "=======================================================================================\n",
            "Epoch: 97/100\tBatch: 100/299\tLoss: 0.6239023208618164\n",
            "Epoch: 97/100\tBatch: 200/299\tLoss: 0.6357776522636414\n",
            "=======================================================================================\n",
            "Epoch: 98/100\tBatch: 100/299\tLoss: 0.6264145374298096\n",
            "Epoch: 98/100\tBatch: 200/299\tLoss: 0.6204940676689148\n",
            "=======================================================================================\n",
            "Epoch: 99/100\tBatch: 100/299\tLoss: 0.6281473636627197\n",
            "Epoch: 99/100\tBatch: 200/299\tLoss: 0.6202314496040344\n",
            "=======================================================================================\n",
            "Epoch: 100/100\tBatch: 100/299\tLoss: 0.6087771654129028\n",
            "Epoch: 100/100\tBatch: 200/299\tLoss: 0.622975766658783\n",
            "=======================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_data = model.feature_extractor(train_loader)\n",
        "print (extracted_data.shape)\n",
        "print (extracted_data)\n",
        "x_train = extracted_data[:, :-1]\n",
        "y_train = extracted_data[:, -1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npmTOHhCohNp",
        "outputId": "a62258e1-3995-4089-9120-67900b53b9e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(76324, 26)\n",
            "[[2.26482391 0.         0.         ... 0.         0.05540963 0.        ]\n",
            " [0.17587906 0.         0.         ... 0.         0.         1.        ]\n",
            " [0.         0.52901733 0.3473174  ... 0.         0.         1.        ]\n",
            " ...\n",
            " [2.41943145 0.         0.         ... 0.         0.         0.        ]\n",
            " [1.86390448 0.         0.06162177 ... 0.89079309 0.         0.        ]\n",
            " [1.36132765 0.24491873 0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_data = model.feature_extractor(test_loader)\n",
        "print (extracted_data.shape)\n",
        "print (extracted_data)\n",
        "x_test = extracted_data[:, :-1]\n",
        "y_test = extracted_data[:, -1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DBMP605ogwW",
        "outputId": "a517e4d2-632b-43db-846b-eee96a2786d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25442, 26)\n",
            "[[1.75885141 0.         0.11609032 ... 0.         0.         0.        ]\n",
            " [1.93714237 0.42939928 0.40308356 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         1.        ]\n",
            " ...\n",
            " [1.69368577 0.         0.38274473 ... 0.         0.         0.        ]\n",
            " [2.53201723 0.         0.         ... 0.09034063 0.49434343 1.        ]\n",
            " [1.82230234 0.61023349 0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "lsvm = SVC()\n",
        "lsvm.C = 1\n",
        "lsvm = lsvm.fit(x_train, y_train)\n",
        "print (lsvm.score(x_train, y_train))\n",
        "print (lsvm.score(x_test, y_test))"
      ],
      "metadata": {
        "id": "VyKIohS_klxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dcf2b44-0176-4764-b78e-1774eb243c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6517609139982181\n",
            "0.6386683436836726\n"
          ]
        }
      ]
    }
  ]
}